\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{caption}
\pagestyle{fancy}
\lhead{Gomez, Harmon}
\chead{DA Final Report}
\rhead{May 3, 2017}
\setlength{\headheight}{20pt}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\setlength{\parindent}{0pt}


\begin{document}

\title{Data Analysis Final Report}
\author{Justin Gomez and Paul Harmon}
\date{May 3, 2017}
\maketitle

\begin{abstract}
Winning College Basketball programs benefit from increased exposure, funding opportunities, and prestige. With a great deal riding on the success of the team's season, we analyze which aspects of the game contribute to the probability of a home team winning a game. We fit a logistic regression model to analyze data from 16,000 college basketball games since the 2003 season in order to determine which variables were most strongly associated with the probability of the home team winning. \par
Results indicated evidence that Field Goal Percentage and Three Point Percentage Differentials were negatively associated with the probability that the home team would win a given game. Free Throw Percentage, Assist, and Block Differentials were positively associated with the probability of the home team winning. Finally, we find evidence that the effects of several variables, including FG Percentage, Total Rebounds, and Turnovers can change in games that go to overtime.  \\
\end{abstract}

\doublespacing
\section{Introduction}
\subsection{College Basketball: A Growing Sport}
NCAA College Basketball is a major cornerstone of college sports. While the games themselves have tremendous economic impact, college basketball's effects reach beyond the court. Broadcasting the tournament cost CBS nearly 10 billion dollars in 2016 (Ogus 2016). The effect on the schools’ academics is palpable, too; Smith (2008) notes that “a college’s profile will increase with big-time athletics, as will the perception of the school itself”. Indeed, some have even contended the increased visibility has impacts on academic metrics at those schools including increased student quality and retention. It is therefore of interest not only to coaches and fans but also to campus administrators to determine which factors are most associated with winning basketball games.

\subsection{Predictive Analytics in NCAA Basketball}
Fans watching the NCAA Tournament are increasingly interested in making bets by filling out brackets and predicting each winner in the seven rounds of the tournament; this has led to a proliferation of predictive modeling in college basketball, both academic and casual. Some methods like those used by Harville and Smith (1992) make use of linear models to predict outcomes of games; however, more recent focus has been on so-called ``black box" predictive analytics. This analysis is focused more on identifying metrics that are associated with the probability of a team winning a game; therefore, we use generalized linear models to analyze the data. The main questions of interest are as follows:

\begin{enumerate}
\item %1
Which factors are most important in helping a home team win a game?
\item %2
Do the effects of each covariate on the probability of winning change in games that go to overtime?
\item %3
Can winning at home be accurately predicted?
\end{enumerate}

We focus mainly on the development of a binary regression model in order determine which factors may influence the probability of a home team winning a given game, and thus the main goal of this analysis is statistical inference. However, since we are developing a model for the probability of the home team winning, evaluating the model's predictive ability is also of secondary interest. To do this, we split our full data set into two sets: a training set comprised of 70\% of the data, obtained through stratified random sampling by season; and a testing set comprised of the remaining 30\% of the data. We fit our generalized linear models on the training set, and then predict whether home teams won or lost using our testing data. Since we know the actual outcome of each game, we can find classification rates for our predictions to judge the performance of the model.

\section{Data and Methods}

\subsection{The Data}
The data were obtained from a repository managed by Kaggle, Inc. related to the March Machine Learning Mania competition in 2016. We settled on a reduced set of ten predictor variables to include in our study of our single binary response: whether the home team won or not. Our dataset contains more than sixty-four thousand games spanning fourteen seasons of basketball. For each game, we can have a slew of performance statistics for both teams summarized as differences (differentials) in percentages or raw counts. For percentage predictors, we have field goal, three-point, and free-throw percentages for both the winning and the losing teams, and will include this information in the model by subtracting the losing team values from the winning team values. All percentages were calculated as the number of made shots divided by the number of attempted shots for that specific shot type. For raw counts, we will be considering total rebounds (offensive rebounds plus defensive rebounds), assists, turnovers, steals, blocks, and personal fouls for both the winning and losing teams. Again, differentials are found for each count as the winning team's numbers minus the losing team's numbers. For instance, a positive rebound differential means that the winning team had more total rebounds than the losing team; a negative value would imply the opposite. Whether or not a game goes into overtime is of interest as well as it may provide some insight into the level of competition between the teams.

\subsection{Generalized Linear Models}
The response in this analysis is binary; either the home team won the game and is assigned a value of ``1" or the home team lost the game, in which case it is assigned a ``0". The binary nature of this response variable allows us to perform logistic regression using the logit link function. In the context of our study, the logit is the log odds of the home team winning. The logit link is used as it is the canonical link function, which has properties that we like to take advantage of when fitting these models as discussed in Section 2.3. Using this generalized linear model allows us to model the probability that the home team wins as function of our suite of predictor variables. Equation~\ref{eq:logit} is the general form of the model that we will be fitting.

\begin{equation} \label{eq:logit}
logit(\pi_{i})=log\Big(\frac{\pi_{i}}{1-\pi_{i}}\Big)=\beta_{0}+\sum_{j=1}^{p} \beta_{j}x_{ij}
\end{equation}

In this equation, $\pi_{i}$ is the probability that the home team wins in the $i^{th}$ game, the $\beta_{j}$'s are the parameters that we are interested in estimating, and the $x_{ij}$'s are our covariates. Using the statistical software R (R Core Team 2017), we can fit a logistic regression equation using the function \texttt{glm} in the \textt{stats} package. With this function, we can specify the predictor variables to be used, as well as the family the response follows and the link function to be used.

\subsection{Link Functions}
In generalized linear models, the link function relates the linear predictors (the $x_{ij}$) to the response. The choice of link function plays an important role in the interpretation of the model. Two choices are common: the logit link and the probit link.  We choose the logit link, which allows for direct calculation of the probability of the home team winning a game. The probit link does not make a great deal of sense in this model because we have little reason to develop an underlying latent scale of player performance; it is more useful to coaches and players to understand the relationship between on-court performance as measured and the effect it has on the probability of winning.

\subsection{Descriptive Statistics}

\subsubsection{Differentials: Percentages}
The percentage variables give some interesting information about the teams that won at home.  In general, we would expect that teams that win games would tend to make a higher percentage of their shots attempted; Table~\ref{tab:percentsum} and Figure~\ref{fig:percentbox} indicate that this is the case, in general. For these three variables, we see that the winning team does not always perform better (indicated by the negative values we see for the minimums). For field goals, this happens less than 25\% of the time, but for three pointers and free throws, this happens just over 25\% of the time. The boxplots give no indication of skew for any of the three variables, but we do see a great deal of variability in the free throw percentage variable.

\begin{table}[h]
\centering
\begin{tabular}{rrrr}
  \hline
 & Field Goals & Three Pointers & Free Throws \\ 
  \hline
Minimum & -0.29 & -0.60 & -0.86 \\ 
  Median & 0.07 & 0.07 & 0.04 \\ 
  Mean & 0.07 & 0.07 & 0.04 \\ 
  Maximum & 0.44 & 0.73 & 1.00 \\ 
   \hline
\end{tabular}
\caption{\label{tab:percentsum} Table of summary statistics for the percentage variables.}
\end{table}

\vspace{10pt}
\begin{figure}[h]
\centering
\includegraphics[width=.75\textwidth]{percentbox.jpeg}
\caption{\label{fig:percentbox} Boxplots for the three percentage variables.}
\end{figure}

\subsubsection{Differentials: Counts}
Recall that each differential refers to the difference between the number of the given statistic for the winning team subtracted by the value for the losing team. Table~\ref{tab:diffsum} and Figure~\ref{fig:diffbox} help us understand the differences in the six count differentials.

\begin{table}[h]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & Turnovers & Assists & Steals & Blocks & Personal Fouls & Total Rebounds \\ 
  \hline
Minimum & -31.00 & -17.00 & -18.00 & -16.00 & -24.00 & -31.00 \\ 
  Median & -1.00 & 3.00 & 1.00 & 1.00 & -2.00 & 4.00 \\ 
  Mean & -1.42 & 3.33 & 1.07 & 1.01 & -2.42 & 3.85 \\ 
  Maximum & 20.00 & 31.00 & 22.00 & 19.00 & 20.00 & 48.00 \\ 
   \hline
\end{tabular}
\caption{\label{tab:diffsum} Table of summary statistics for the six raw count variables.}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=.75\textwidth]{diffbox.jpeg}
\caption{\label{fig:diffbox} Boxplots for the six raw count variables.}
\end{figure}

It should first be pointed out that negative numbers are expected for turnovers and personal fouls as the winning team is expected to turn the ball over less and commit fewer personal fouls. These summaries statistics in Table~\ref{tab:diffsum} indicate much of what would be expected; however, there are a few values that indicate some fascinating game results. At some point, a team that was out-rebounded by thirty-one rebounds managed to win the game; moreover, a team that turned the ball over twenty times also managed to escape with a win. For the two covariates that we expect to be negative, we see negative means. Again, mean and median values are close, and the boxplots do not indicate heavy skew for any variable. Total rebounds contains the most variablility of these six, and blocks contains the least variability. Taken together, these statistics imply that there is no sure-fire way to win a basketball game. A team can out-rebound its opponent, it can win the turnover battle, or dominate its opponent on any single statistic; however, without a well-rounded performance on all metrics, it can be very possible to lose. 

\subsection{Logistic Regression}
In order to fit a model to answer our research questions, we fit several models considering a variety of main effects and interactions. The final model was selected through an AIC comparison. A Hosmer-Lemeshow goodness-of-fit test was also conducted on the final model. This test yielded a test-statistic of 10.291 on eight degrees of freedom, and with an associated p-value of 0.2452, there is no evidence that the model is an inadequate fit for the data. The theoretical model following the form of Equation~\ref{eq:logit} is given in Equation~\ref{eq:finalmodel}, and the results of the regression are shown in Table ~\ref{tab:regression}. 

\begin{multline} \label{eq:finalmodel}
logit(\pi_{i})=\beta_{0}+\beta_{1}ot_{i}+\beta_{2}fgp_{i}+\beta_{3}tpp_{i}+\beta_{4}ftp_{i}+\beta_{5}todiff_{i}+\beta_{6}astdiff_{i}+\beta_{7}stldiff_{i}\\
+\beta_{8}blkdiff_{i}+\beta_{9}pfdiff_{i}+\beta_{10}ot_{i}\times fgp_{i}+\beta_{11}ot_{i}\times todiff_{i} +\beta_{12}ot_{i}\times trdiff_{i}
\end{multline}

\begin{table}[h]
\centering
\begin{tabular}{rrrrr}
  \hline
 Term & Abbreviation & Lower Bound & Estimate & Upper Bound \\ 
  \hline
Intercept & & 0.78 & 0.81 & 0.84 \\ 
  Overtime & ot & 0.99 & 1.10 & 1.23 \\ 
  Field Goal \% & fgp & 0.18 & 0.26 & 0.37 \\ 
  Three Point \% & tpp & 0.57 & 0.67 & 0.79 \\ 
  Free Throw \% & ftp & 1.37 & 1.56 & 1.78 \\ 
  Turnovers & todiff & 0.89 & 0.90 & 0.90 \\ 
  Assists & astdiff & 1.13 & 1.14 & 1.15 \\ 
  Steals & stldiff & 0.97 & 0.97 & 0.98 \\ 
  Blocks & blkdiff & 1.15 & 1.16 & 1.17 \\ 
  Personal Fouls & pfdiff & 0.86 & 0.87 & 0.87 \\ 
  Total Rebounds & trdiff & 1.03 & 1.03 & 1.04 \\ 
  Overtime:Field Goal \% & ot\times fgp & 0.00 & 0.02 & 0.12 \\ 
  Overtime:Turnovers & ot\times todiff & 1.03 & 1.06 & 1.09 \\ 
  Overtime:Total Rebounds & ot\times trdiff & 0.96 & 0.97 & 0.99 \\ 
   \hline
\end{tabular}
\caption{\label{tab:regression} Regression table summarizing the estimated odds ratios from the fitted generalized linear model and bounds for the associated 95\% confidence intervals.}
\end{table}

\subsection{Factors Related to Winning}
The estimates in the regression table give the estimated odds ratio that the home team won a given game. Confidence intervals that contain 1 imply that the true odds do not differ for different levels of a given predictor; this is considered a lack of evidence against the fact that the predictor is associated with changes in the probability of the home team winning the game. Because of the large sample size, relying on statistical significance as denoted by small p-values to determine which factors are important is not reasonable; the most important predictors should also have large magnitude effects. 

The results of the logistic regression shed some light on some fascinating findings. First, we find evidence that the effects of three variables differ in games that go to overtime vs. those that do not. Those variables are Field Goal percentage, Assists, and Turnovers. For instance, given the results we found, in games that did not go to overtime, and holding other variables constant, in games where the winning team had an additional turnover relative to the losing team, the odds of the home team winning decreased by between 10 and 11 percent. If the game went to overtime, that decrease in the odds of the home team winning actually increased relative to non-OT games by 3 to 9 percent.

The variables that had the largest-magnitude effects on the odds of the home team winning the game were Field Goal Percentage differential, Three Point Percentage differential, Assists differential, and Free Throw differential. Both Field Goal Percentage and Three Point Differentials were negatively associated with winning. Holding the other variables constant (regardless of overtime status), in games where the winning team had a one point increase in the Three-Point Differential  relative to the losing team, the estimated odds that the home team was the winner actually decrease by between 21 and 43 percent. This seems unintuitive; however, there may be some issues with multicollinearity in the data. Even if that is not the case, both Field Goals and Three Point Percentage variables do not consider how many shots were taken, so high percentages may be reflective that few fewer shots were actually attempted by the winning team. The more intuitive result comes from the Free Throws variable. For a one-unit increase in Free Throw differential, the estimated odds of the home team winning the game increase by 27 and 78 percent. This underscores the importance of making free throws in College Basketball.  

\subsection{Prediction} 
The goals for this analysis are primarily inferential in nature; however, it is worthwhile to examine the predictive efficacy of the model for picking winners and losers of any given home game. We partitioned the original data into a training set on which we fit statistical models and a testing set on which we tested our predictions from the final model. The testing set had 14276 games. The final model correctly classified 72.88\% of the games as wins. This classification rate is better than a coin flip, but not good enough to be used in a reliable way. More sophisticated methods would likely need to be employed to generate better classifications.


\section{Discussion}
The regression analysis indicates much of what we already know. Teams that score more points, turn the ball over less, and cut down on fouls tend to be more likely to win basketball games. In reference to the first question of interest, the factors that are most highly associated with winning had both small p-values and large effects on the odds of winning. Ultimately, higher Free Trow Percentages Assist, and Block Differentials were the most strongly and positively associated with the odds of the home team winning, with Field Goal and Three Point Percentage Differentials being negatively associated with the odds of the home team winning. 

As an answer to question two, we have evidence that the effects of Field Goal Percentage Differential, Turnover Differential, and Total Rebound Differential appear to change in games that go to overtime.  For the final question, the inferential model predicted about 3 out of every 4 games correctly. Finally, these results should not be interpreted as causal. This methodology does not, however, illuminate how simultaneous increases on multiple covariates might impact the probability of winning, nor does it guarantee that a team that maximizes important predictors of winning will win. Because this analysis pertains only to games played at home, games inferences cannot be made to games played on neutral sites and should therefore not be used for prediction of NCAA Tournament games. 
\newpage
\section{References}
\begin{thebibliography}

\bibitem{Agresti}
Agresti, A. (2015). \textit{Foundations of Linear and Generalized Linear Models}. Somerset: Wiley.

\bibitem{Hsmith}
Harville, D., & Smith, M. (1994). \textit{The Home-Court Advantage: How Large Is It, and Does It Vary from Team to Team?} The American Statistician, 48(1), 22-28. doi:10.2307/2685080
\bibitem{ogus}
Ogus, Simon. (2016) \textit{The Economic Impact of March Madness From First Four to Final Four}. Forbes.

\bibitem{smith}
Smith, D. Randall. (2008) \textit{Big Time College Basketball and the Advertising Effect: Does Success Really Matter?} Journal of Sports Economics Volume 9 No. 4 pp 387-406.




\end{thebibliogrpahy}
\newpage
\section{Appendix - R code}
<<dataanalysis, eval = FALSE>>=
#packages
library(xtable)
library(ResourceSelection)

#bring in data, work into final form
train<-read.csv("train.csv",header=TRUE)[,-c(1,2,20)]
miss<-rep(12,ncol(train))
for(i in 1:ncol(train)) {
  miss[i]<-length(which(is.na(train[,i])))
}
train<-train[-c(which(is.na(train[,11]))),]

test<-read.csv("test.csv",header=TRUE)[,-c(1,2,20)]
miss<-rep(12,ncol(test))
for(i in 1:ncol(test)) {
  miss[i]<-length(which(is.na(test[,i])))
}
test<-test[-c(which(is.na(test[,11]))),]
#missing data has been removed

#data sets for nice labels
train1<-train[,c(9:11)] #for percentages
colnames(train1)<-c("Field Goals","Three Pointers","Free Throws")
train2<-train[,c(12:17)] #for raw diffs
colnames(train2)<-c("Turnovers","Assists","Steals","Blocks","Personal Fouls","T Rebounds")

#boxplots for explanatory vars
boxplot(train1,col=c("midnightblue","darkgreen","gold3"),cex.axis=1.5,ylab="Difference in Percent",cex.lab=1.5) 
abline(h=0,lwd=3,lty=2) #named percentbox
boxplot(train2,at=seq(1,10.5,1.75),col=c("darkred","gold2","blue3","purple","white","orangered"),xaxt="n",cex.axis=1.5,ylab="Difference in Count",cex.lab=1.5) #named diffbox
xtick<-seq(1,10.5,1.75)
axis(side=1,at=xtick,labels=FALSE)
text(x=xtick,par("usr")[3],labels=colnames(train2),srt=13,pos=1,xpd=TRUE,cex=1.5,offset=1)
abline(h=0,lwd=3,lty=2)

#plots for winning/losing at home
rtrain<-train[,c(7:17)]
par(mfrow=c(3,3),mai = c(0.4, 0.3, 0.2, 0.1))
titles<-c(colnames(train1),colnames(train2))
for(i in 3:ncol(rtrain)) {
boxplot(rtrain[,i]~winloss,data=train,xaxt="n",cex.axis=1.5,main=titles[i-2],col=c("lightblue","lightcoral"))
abline(h=0,lwd=3,lty=2)
xtick<-seq(1,2,1)
axis(side=1,at=xtick,labels=FALSE)
text(x=xtick,par("usr")[3],labels=c("Loss","Win"),srt=0,pos=1,xpd=TRUE,cex=1.5,offset=1)
} #saved as toomanyboxes

#numerical summaries for explanatory vars
summary(train1)
summary(train2)

fg<-c(min(train1[,1]),median(train1[,1]),mean(train1[,1]),max(train1[,1]))
tp<-c(min(train1[,2]),median(train1[,2]),mean(train1[,2]),max(train1[,2]))
ft<-c(min(train1[,3]),median(train1[,3]),mean(train1[,3]),max(train1[,3]))
tab1<-data.frame(fg=round(fg,3),tp=round(tp,3),ft=round(ft,3))
colnames(tab1)<-colnames(train1)
rownames(tab1)<-c("Minimum","Median","Mean","Maximum")
xtable(tab1) #table for percent summary stats

to<-c(min(train2[,1]),median(train2[,1]),mean(train2[,1]),max(train2[,1]))
ast<-c(min(train2[,2]),median(train2[,2]),mean(train2[,2]),max(train2[,2]))
stl<-c(min(train2[,3]),median(train2[,3]),mean(train2[,3]),max(train2[,3]))
blk<-c(min(train2[,4]),median(train2[,4]),mean(train2[,4]),max(train2[,4]))
pf<-c(min(train2[,5]),median(train2[,5]),mean(train2[,5]),max(train2[,5]))
tr<-c(min(train2[,6]),median(train2[,6]),mean(train2[,6]),max(train2[,6]))
tab2<-data.frame(to=round(to,3),ast=round(ast,3),stl=round(stl,3),blk=round(blk,3),
                 pf=round(pf,3),tr=round(tr,3))
colnames(tab2)<-colnames(train2)
rownames(tab2)<-c("Minimum","Median","Mean","Maximum")
xtable(tab2) #table for diff summary stats

#summaries for x vars with response
tab3<-addmargins(table(as.factor(train$winloss),as.factor(train$ot)))
colnames(tab3)<-c("Regulation","Overtime","Total")
rownames(tab3)<-c("Loss","Won","Total")
xtable(tab3) #table for win/loss by overtime

#model time
train$ot<-as.factor(train$ot)
test$ot<-as.factor(test$ot)
mod1<-glm(winloss~ot*fgp+ot*tpp+ot*ftp+ot*todiff+ot*astdiff+ot*stldiff+
               ot*blkdiff+ot*pfdiff+ot*trdiff,data=train,family=binomial)
mod2<-update(mod1,.~.-ot:tpp-ot:ftp-ot:astdiff-ot:stldiff-ot:blkdiff-ot:pfdiff,
             data=train)
anova(mod2,mod1,test="LRT") #looks like we want them model with 3 interactions
#what about additive effects model?
mod3<-update(mod2,.~.-ot:fgp-ot:todiff-ot:trdiff,data=train)
anova(mod3,mod2,test="LRT") #only main effects would be a bad idea I guess

tab4<-AIC(mod2,mod1,mod3) #model 2 wins!
tab4<-data.frame(tab4,diff=c(tab4[1,2]-tab4[1,2],tab4[2,2]-tab4[1,2],tab4[3,2]-tab4[1,2]))
rownames(tab4)<-c("Reduced Interaction Model","Full Interaction Model","Additive Model")
colnames(tab4)<-c("df","AIC","Change in AIC")
xtable(tab4)

hoslem.test(mod2$y,fitted(mod2),g=10)

ci<-confint(mod2)
tab5<-data.frame(low=round(exp(ci[,1]),3),est=round(exp(summary(mod2)$coefficients[,1]),3),up=round(exp(ci[,2]),3))
colnames(tab5)<-c("Lower Bound","Estimate","Upper Bound")
rownames(tab5)<-c("Intercept","Overtime","Field Goal %","Three Point %","Free Throw %","Turnovers","Assists","Steals","Blocks","Personal Fouls","Total Rebounds","Overtime:Field Goal %","Overtime:Turnovers","Overtime:Total Rebounds")
xtable(tab5) #table for estimated odds ratios and 95% CI

misclass<-function(mod){
  p<-predict(mod,newdata=test,type="response")
  for(i in 1:length(p)){
    ifelse(p[i]>.5,p[i]<-1,p[i]<-0)
  }
  tab<-table(p,test$winloss)
  mis<-1-sum(diag(tab)/sum(tab))
}

mis1<-misclass(mod1)
mis2<-misclass(mod2)
mis3<-misclass(mod3)

@
\end{document}